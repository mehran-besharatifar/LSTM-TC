{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b9c2c9-b128-4253-86d5-9d0d1e509167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "def blending_weights(errors):\n",
    "    \"\"\"\n",
    "    Compute weights for each dataset using inverse squared errors.\n",
    "    Errors with smaller variance get higher weights.\n",
    "    \"\"\"\n",
    "    inv_sq = 1 / (errors ** 2 + 1e-10)\n",
    "    sum_inv_sq = np.sum(inv_sq, axis=0, keepdims=True)\n",
    "    return inv_sq / sum_inv_sq\n",
    "\n",
    "\n",
    "def blend_estimates(estimates, errors):\n",
    "    \"\"\"\n",
    "    Blend multiple dataset estimates into one corrected product.\n",
    "    Also compute the overall uncertainty of the blended estimate.\n",
    "    \"\"\"\n",
    "    weights = blending_weights(errors)\n",
    "    blended_estimate = np.sum(weights * estimates, axis=0)\n",
    "    blended_uncertainty = 1 / np.sum(1 / (errors ** 2 + 1e-10), axis=0)\n",
    "    return blended_estimate, blended_uncertainty\n",
    "\n",
    "\n",
    "def simulate_precipitation_datasets(T=800, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic precipitation datasets and their errors\n",
    "    to simulate satellite/rain gauge uncertainty for TC.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    true_precip = np.sin(np.linspace(0, 20, T)) + np.random.normal(0, 0.05, size=T)\n",
    "    datasets, errors = [], []\n",
    "    for i in range(3):  # simulate 3 input datasets\n",
    "        noise = np.random.normal(0, 0.3 + 0.1 * i, size=T)\n",
    "        x_i = true_precip + noise\n",
    "        err = np.abs(noise)\n",
    "        datasets.append(x_i)\n",
    "        errors.append(err)\n",
    "    return true_precip, np.array(datasets), np.array(errors)\n",
    "\n",
    "\n",
    "\n",
    "def create_sequences(data, targets, seq_length):\n",
    "    \"\"\"\n",
    "    Convert time series data into LSTM sequences.\n",
    "    Each input sequence has length `seq_length`.\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for t in range(len(data) - seq_length):\n",
    "        X_seq.append(data[t:t+seq_length, :])\n",
    "        y_seq.append(targets[t+seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "\n",
    "def normalize(data):\n",
    "    \"\"\"\n",
    "    Normalize features to zero mean and unit variance.\n",
    "    \"\"\"\n",
    "    mean, std = data.mean(axis=0), data.std(axis=0) + 1e-6\n",
    "    return (data - mean) / std, mean, std\n",
    "\n",
    "\n",
    "\n",
    "class CustomLSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom implementation of a single LSTM cell\n",
    "    with input, forget, output gates, and cell state.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CustomLSTMCell, self).__init__()\n",
    "        # Input gate\n",
    "        self.W_i, self.U_i, self.b_i = nn.Linear(input_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim, bias=False), nn.Parameter(torch.zeros(hidden_dim))\n",
    "        # Forget gate\n",
    "        self.W_f, self.U_f, self.b_f = nn.Linear(input_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim, bias=False), nn.Parameter(torch.zeros(hidden_dim))\n",
    "        # Candidate cell state\n",
    "        self.W_c, self.U_c, self.b_c = nn.Linear(input_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim, bias=False), nn.Parameter(torch.zeros(hidden_dim))\n",
    "        # Output gate\n",
    "        self.W_o, self.U_o, self.b_o = nn.Linear(input_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim, bias=False), nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "    def forward(self, x_t, h_prev, c_prev):\n",
    "        i_t = torch.sigmoid(self.W_i(x_t) + self.U_i(h_prev) + self.b_i)\n",
    "        f_t = torch.sigmoid(self.W_f(x_t) + self.U_f(h_prev) + self.b_f)\n",
    "        c_tilde = torch.tanh(self.W_c(x_t) + self.U_c(h_prev) + self.b_c)\n",
    "        c_t = f_t * c_prev + i_t * c_tilde\n",
    "        o_t = torch.sigmoid(self.W_o(x_t) + self.U_o(h_prev) + self.b_o)\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        return h_t, c_t\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Full LSTM sequence model built from CustomLSTMCell.\n",
    "    Predicts precipitation at final time step.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, sequence_length):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.lstm_cell = CustomLSTMCell(input_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x_seq):\n",
    "        batch_size = x_seq.size(0)\n",
    "        h_t, c_t = torch.zeros(batch_size, self.hidden_dim), torch.zeros(batch_size, self.hidden_dim)\n",
    "        for t in range(self.sequence_length):\n",
    "            x_t = x_seq[:, t, :]\n",
    "            h_t, c_t = self.lstm_cell(x_t, h_t, c_t)\n",
    "        return self.fc(h_t).squeeze(-1)\n",
    "\n",
    "\n",
    "def train_lstm(model, X_train, y_train, epochs=200, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train the LSTM model using MSE loss and Adam optimizer.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    criterion, optimizer = nn.MSELoss(), optim.Adam(model.parameters(), lr=lr)\n",
    "    dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                            torch.tensor(y_train, dtype=torch.float32))\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for xb, yb in loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "        avg_loss = total_loss / len(dataset)\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1} Loss: {avg_loss:.4f}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute standard evaluation metrics: RMSE, MAE, R2.\n",
    "    \"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {\"RMSE\": rmse, \"MAE\": mae, \"R2\": r2}\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Step 1: Simulate raw datasets\n",
    "    true_precip, datasets, errors = simulate_precipitation_datasets(T=800)\n",
    "\n",
    "    # Step 2: Apply Triple Collocation blending\n",
    "    tc_precip, tc_uncertainty = blend_estimates(datasets, errors)\n",
    "\n",
    "    # Step 3: Build feature set (precip + auxiliaries)\n",
    "    auxiliaries = np.random.randn(len(tc_precip), 3)  # simulate other predictors\n",
    "    features = np.column_stack((tc_precip, auxiliaries))\n",
    "    features, _, _ = normalize(features)\n",
    "    target = tc_precip\n",
    "\n",
    "    # Step 4: Create LSTM sequences\n",
    "    seq_length = 10\n",
    "    X_seq, y_seq = create_sequences(features, target, seq_length)\n",
    "\n",
    "    # Step 5: Train/test split\n",
    "    split_idx = int(0.8 * len(X_seq))\n",
    "    X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
    "    y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
    "\n",
    "    # Step 6: Initialize and train LSTM\n",
    "    input_dim, hidden_dim = features.shape[1], 64\n",
    "    model = LSTMModel(input_dim, hidden_dim, 1, seq_length)\n",
    "    model = train_lstm(model, X_train, y_train, epochs=200)\n",
    "\n",
    "    # Step 7: Predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(torch.tensor(X_test, dtype=torch.float32)).numpy()\n",
    "\n",
    "    # Step 8: Evaluation\n",
    "    metrics = evaluate(y_test, y_pred)\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "    # Step 9: Display sample predictions\n",
    "    print(\"\\nSample Results:\")\n",
    "    print(\"Observed:\", y_test[:10])\n",
    "    print(\"Predicted:\", y_pred[:10])\n",
    "    print(\"Uncertainty:\", tc_uncertainty[seq_length:seq_length+10])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
